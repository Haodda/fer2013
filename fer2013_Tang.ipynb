{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.read data\n",
    "path = \"D:/python/contest/picture/fer2013/\"\n",
    "file = \"fer2013.csv\"\n",
    "data = pd.read_csv(path+file)\n",
    "data['pixels'] = data['pixels'].apply(lambda x: np.array([int(pixel) for pixel in x.split(' ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.split data into sets\n",
    "train_set   = data.loc[data['Usage'] == 'Training',['emotion','pixels']].reset_index(drop=True)\n",
    "public_set  = data.loc[data['Usage'] == 'PublicTest',['emotion','pixels']].reset_index(drop=True)\n",
    "private_set = data.loc[data['Usage'] == 'PrivateTest',['emotion','pixels']].reset_index(drop=True)\n",
    "# use train_set to train\n",
    "# use public_set to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.make batch generator\n",
    "#  using generator to save the memory\n",
    "def sample_generator(df,batch_size):\n",
    "    while True:\n",
    "        index = np.arange(df.shape[0]) # get index of all picture\n",
    "        random.shuffle(index)          # shuffle the index\n",
    "        for i in range(0,int(np.ceil(df.shape[0]/batch_size))):\n",
    "            id1 = i*batch_size\n",
    "            id2 = min((i+1)*batch_size,len(index))\n",
    "            batch_id = index[id1:id2]\n",
    "            batch_X  = df.loc[batch_id,'pixels'].tolist()\n",
    "            batch_Y  = df.loc[batch_id,'emotion'].tolist()\n",
    "            #make the batch data to array format            \n",
    "            batch_X  = [x.reshape(1,48,48,1) for x in batch_X]\n",
    "            #regularize the data\n",
    "            batch_X  = [x/128-1 for x in batch_X]\n",
    "            batch_X  = np.concatenate(batch_X,axis=0)\n",
    "            batch_Y  = to_categorical(batch_Y,num_classes=7)\n",
    "            yield batch_X , batch_Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.build model by keras\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "model.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(64, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,split):\n",
    "    index = np.arange(df.shape[0]) # get index of all picture\n",
    "    random.shuffle(index)\n",
    "    split_id = round(df.shape[0]*split)\n",
    "    train_index,test_index = index[:split_id],index[split_id:]\n",
    "    train_df = df.loc[train_index].copy(deep=True).reset_index(drop=True)\n",
    "    test_df  = df.loc[test_index].copy(deep=True).reset_index(drop=True)\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 4s - loss: 1.7588 - acc: 0.2759 - val_loss: 1.6225 - val_acc: 0.3595\n",
      "Epoch 2/10\n",
      " - 3s - loss: 1.5061 - acc: 0.4179 - val_loss: 1.4659 - val_acc: 0.4316\n",
      "Epoch 3/10\n",
      " - 3s - loss: 1.3703 - acc: 0.4785 - val_loss: 1.3650 - val_acc: 0.4781\n",
      "Epoch 4/10\n",
      " - 3s - loss: 1.2796 - acc: 0.5158 - val_loss: 1.2788 - val_acc: 0.5206\n",
      "Epoch 5/10\n",
      " - 3s - loss: 1.2040 - acc: 0.5483 - val_loss: 1.2499 - val_acc: 0.5283\n",
      "Epoch 6/10\n",
      " - 3s - loss: 1.1516 - acc: 0.5667 - val_loss: 1.2442 - val_acc: 0.5315\n",
      "Epoch 7/10\n",
      " - 3s - loss: 1.1031 - acc: 0.5858 - val_loss: 1.2224 - val_acc: 0.5416\n",
      "Epoch 8/10\n",
      " - 3s - loss: 1.0398 - acc: 0.6118 - val_loss: 1.2244 - val_acc: 0.5347\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.9898 - acc: 0.6348 - val_loss: 1.2227 - val_acc: 0.5408\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.9425 - acc: 0.6548 - val_loss: 1.2516 - val_acc: 0.5391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bca370a828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.train\n",
    "batch_size = 128\n",
    "# train_test_split\n",
    "train_train,train_test = train_test_split(train_set,0.7)\n",
    "model.fit_generator(sample_generator(train_train,batch_size),\n",
    "                    steps_per_epoch = np.ceil(train_train.shape[0]/batch_size),\n",
    "                    validation_data = sample_generator(train_test,batch_size),\n",
    "                    validation_steps= np.ceil(train_test.shape[0]/batch_size),\n",
    "                    epochs = 10,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public accuracy:0.5397046531399301\n"
     ]
    }
   ],
   "source": [
    "#6.evaluate\n",
    "result = model.evaluate_generator(sample_generator(public_set,batch_size),\n",
    "                         steps=np.ceil(public_set.shape[0]/batch_size))\n",
    "print(f\"public accuracy:{result[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
